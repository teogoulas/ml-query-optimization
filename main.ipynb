{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from models.database import Database\n",
    "from models.embeddings_model import EmbeddingsModel\n",
    "from models.job_query import JOBQuery\n",
    "from utils.downloader import get_glove_vectors\n",
    "from utils.parser import generate_output_text, generate_input_text\n",
    "from utils.vectorization import text_vectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate training-test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed parse where statement:  cn.country_code ='[us]'\n",
      "  AND ct.kind IS NOT NULL\n",
      "  AND (ct.kind ='production companies'\n",
      "       OR ct.kind = 'distributors')\n",
      "  AND it1.info ='budget'\n",
      "  AND it2.info ='bottom 10 rank'\n",
      "  AND t.production_year >2000\n",
      "  AND (t.title LIKE 'Birdemic%'\n",
      "       OR t.title LIKE '%Movie%')\n",
      "  AND t.id = mi.movie_id\n",
      "  AND t.id = mi_idx.movie_id\n",
      "  AND mi.info_type_id = it1.id\n",
      "  AND mi_idx.info_type_id = it2.id\n",
      "  AND t.id = mc.movie_id\n",
      "  AND ct.id = mc.company_type_id\n",
      "  AND cn.id = mc.company_id\n",
      "  AND mc.movie_id = mi.movie_id\n",
      "  AND mc.movie_id = mi_idx.movie_id\n",
      "  AND mi.movie_id = mi_idx.movie_id\n",
      "Failed parse where statement:  ct.kind = 'production companies'\n",
      "  AND it.info = 'top 250 rank'\n",
      "  AND mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%'\n",
      "  AND (mc.note LIKE '%(co-production)%')\n",
      "  AND t.production_year >2010\n",
      "  AND ct.id = mc.company_type_id\n",
      "  AND t.id = mc.movie_id\n",
      "  AND t.id = mi_idx.movie_id\n",
      "  AND mc.movie_id = mi_idx.movie_id\n",
      "  AND it.id = mi_idx.info_type_id\n",
      "Failed parse where statement:  ct.kind = 'production companies'\n",
      "  AND it.info = 'bottom 10 rank'\n",
      "  AND mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%'\n",
      "  AND t.production_year >2000\n",
      "  AND ct.id = mc.company_type_id\n",
      "  AND t.id = mc.movie_id\n",
      "  AND t.id = mi_idx.movie_id\n",
      "  AND mc.movie_id = mi_idx.movie_id\n",
      "  AND it.id = mi_idx.info_type_id\n",
      "Failed parse where statement:  an.name LIKE '%a%'\n",
      "  AND it.info ='mini biography'\n",
      "  AND lt.link ='features'\n",
      "  AND n.name_pcode_cf LIKE 'D%'\n",
      "  AND n.gender='m'\n",
      "  AND pi.note ='Volker Boehm'\n",
      "  AND t.production_year BETWEEN 1980 AND 1984\n",
      "  AND n.id = an.person_id\n",
      "  AND n.id = pi.person_id\n",
      "  AND ci.person_id = n.id\n",
      "  AND t.id = ci.movie_id\n",
      "  AND ml.linked_movie_id = t.id\n",
      "  AND lt.id = ml.link_type_id\n",
      "  AND it.id = pi.info_type_id\n",
      "  AND pi.person_id = an.person_id\n",
      "  AND pi.person_id = ci.person_id\n",
      "  AND an.person_id = ci.person_id\n",
      "  AND ci.movie_id = ml.linked_movie_id\n"
     ]
    }
   ],
   "source": [
    "dataset = \"data/queries\"\n",
    "cwd = os.getcwd()\n",
    "files = os.listdir(os.path.join(cwd, *dataset.split(\"/\")))\n",
    "\n",
    "db = Database(collect_db_info=True)\n",
    "column_array_index = []\n",
    "for table, columns in db.tables_attributes.items():\n",
    "    for column in columns:\n",
    "        column_array_index.append(table + \"_\" + column)\n",
    "\n",
    "# initialize all variables\n",
    "raw_input_texts = []\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "for file in files:\n",
    "    f = open(dataset + \"/\" + file, \"r\")\n",
    "    query = f.read().strip()\n",
    "    raw_input_texts.append(query)\n",
    "    job_query = JOBQuery(query)\n",
    "    rows = db.explain_query(query)\n",
    "\n",
    "    input_text = generate_input_text(job_query.predicates, job_query.rel_lookup)\n",
    "    input_texts.append(input_text)\n",
    "    # add '\\t' at start and '\\n' at end of text.\n",
    "    target_text = '\\t' + generate_output_text(rows, job_query.rel_lookup)[:-1] + '\\n'\n",
    "    target_texts.append(target_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorize inputs & outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    }
   ],
   "source": [
    "raw_input_vectorizer, raw_input_corpus = text_vectorization(pd.DataFrame(raw_input_texts,\n",
    "                                                                         columns=['input_queries']),\n",
    "                                                            ['input_queries'], (1, 3))\n",
    "\n",
    "input_vectorizer, input_corpus = text_vectorization(pd.DataFrame(input_texts, columns=['input_queries']),\n",
    "                                                    ['input_queries'], (1, 1))\n",
    "\n",
    "output_vectorizer, output_corpus = text_vectorization(pd.DataFrame(target_texts, columns=['output_queries']),\n",
    "                                                      ['output_queries'], (1, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of encoder words :  30\n",
      "number of decoder words :  464\n"
     ]
    }
   ],
   "source": [
    "print(\"number of encoder words : \", len(input_vectorizer.vocabulary_.keys()))\n",
    "print(\"number of decoder words : \", len(output_vectorizer.vocabulary_.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train embedding models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "glove_vectors = get_glove_vectors()\n",
    "input_encoder = EmbeddingsModel()\n",
    "input_encoder.build(input_corpus, glove_vectors)\n",
    "output_encoder = EmbeddingsModel()\n",
    "output_encoder.build(output_corpus, glove_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}