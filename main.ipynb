{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.vectorization import text_vectorization, pad_tokenizer, get_embedding_matrix\n",
    "from utils.downloader import get_glove_vectors\n",
    "from utils.parser import generate_input_text, generate_output_text\n",
    "from utils.ploting import plot_embeddings\n",
    "from utils.training import test_step, train_step\n",
    "from models.embeddings_model import EmbeddingsModel\n",
    "from models.database import Database\n",
    "from models.job_query import JOBQuery\n",
    "from models.decoder import Decoder\n",
    "from models.encoder import Encoder\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import logging\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "# from keras.losses import SparseCategoricalCrossentropy\n",
    "# from keras.metrics import SparseCategoricalAccuracy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate training-test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = \"data/queries\"\n",
    "prefix=\"training\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed parse where statement:  cn.country_code ='[us]'\n",
      "  AND ct.kind IS NOT NULL\n",
      "  AND (ct.kind ='production companies'\n",
      "       OR ct.kind = 'distributors')\n",
      "  AND it1.info ='budget'\n",
      "  AND it2.info ='bottom 10 rank'\n",
      "  AND t.production_year >2000\n",
      "  AND (t.title LIKE 'Birdemic%'\n",
      "       OR t.title LIKE '%Movie%')\n",
      "  AND t.id = mi.movie_id\n",
      "  AND t.id = mi_idx.movie_id\n",
      "  AND mi.info_type_id = it1.id\n",
      "  AND mi_idx.info_type_id = it2.id\n",
      "  AND t.id = mc.movie_id\n",
      "  AND ct.id = mc.company_type_id\n",
      "  AND cn.id = mc.company_id\n",
      "  AND mc.movie_id = mi.movie_id\n",
      "  AND mc.movie_id = mi_idx.movie_id\n",
      "  AND mi.movie_id = mi_idx.movie_id\n",
      "Failed parse where statement:  ct.kind = 'production companies'\n",
      "  AND it.info = 'top 250 rank'\n",
      "  AND mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%'\n",
      "  AND (mc.note LIKE '%(co-production)%')\n",
      "  AND t.production_year >2010\n",
      "  AND ct.id = mc.company_type_id\n",
      "  AND t.id = mc.movie_id\n",
      "  AND t.id = mi_idx.movie_id\n",
      "  AND mc.movie_id = mi_idx.movie_id\n",
      "  AND it.id = mi_idx.info_type_id\n",
      "Failed parse where statement:  ct.kind = 'production companies'\n",
      "  AND it.info = 'bottom 10 rank'\n",
      "  AND mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%'\n",
      "  AND t.production_year >2000\n",
      "  AND ct.id = mc.company_type_id\n",
      "  AND t.id = mc.movie_id\n",
      "  AND t.id = mi_idx.movie_id\n",
      "  AND mc.movie_id = mi_idx.movie_id\n",
      "  AND it.id = mi_idx.info_type_id\n",
      "Failed parse where statement:  an.name LIKE '%a%'\n",
      "  AND it.info ='mini biography'\n",
      "  AND lt.link ='features'\n",
      "  AND n.name_pcode_cf LIKE 'D%'\n",
      "  AND n.gender='m'\n",
      "  AND pi.note ='Volker Boehm'\n",
      "  AND t.production_year BETWEEN 1980 AND 1984\n",
      "  AND n.id = an.person_id\n",
      "  AND n.id = pi.person_id\n",
      "  AND ci.person_id = n.id\n",
      "  AND t.id = ci.movie_id\n",
      "  AND ml.linked_movie_id = t.id\n",
      "  AND lt.id = ml.link_type_id\n",
      "  AND it.id = pi.info_type_id\n",
      "  AND pi.person_id = an.person_id\n",
      "  AND pi.person_id = ci.person_id\n",
      "  AND an.person_id = ci.person_id\n",
      "  AND ci.movie_id = ml.linked_movie_id\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "files = os.listdir(os.path.join(cwd, *dataset.split(\"/\")))\n",
    "\n",
    "db = Database(collect_db_info=True)\n",
    "column_array_index = []\n",
    "for table, columns in db.tables_attributes.items():\n",
    "    for column in columns:\n",
    "        column_array_index.append(table + \"_\" + column)\n",
    "\n",
    "# initialize all variables\n",
    "raw_input_texts = []\n",
    "raw_output_texts = []\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "with open(\"sql.log\", \"w\", encoding='utf-8') as logf:\n",
    "    for file in files:\n",
    "        with open(dataset + \"/\" + file, \"r\") as f:\n",
    "            queries = f.read().strip()\n",
    "            for query in queries.split(\";\"):\n",
    "                if len(query) == 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    query = query.replace('\\n', '').strip()\n",
    "                    raw_input_texts.append(query)\n",
    "                    job_query = JOBQuery(query)\n",
    "                    rows = db.explain_query(query)\n",
    "                    raw_output_texts.append(json.dumps(rows))\n",
    "\n",
    "                    input_text = generate_input_text(job_query.predicates, job_query.rel_lookup)\n",
    "                    input_texts.append(input_text)\n",
    "                    # add '\\t' at start and '\\n' at end of text.\n",
    "                    target_text = generate_output_text(rows, job_query.rel_lookup)[:-1]\n",
    "                    target_texts.append(target_text)\n",
    "                except Exception as e:\n",
    "                    logf.write(\"Failed to execute query {0}: {1}\\n\".format(str(query), str(e)))\n",
    "                    db.conn.close()\n",
    "                    db.conn = db.connect()\n",
    "                    if len(input_texts) != len(target_texts):\n",
    "                        input_texts.pop()\n",
    "                    if len(raw_input_texts) != len(raw_output_texts):\n",
    "                        raw_input_texts.pop()\n",
    "                finally:\n",
    "                    pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save inputs & outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame(input_texts, columns=['input_queries'])\n",
    "output_df = pd.DataFrame(target_texts, columns=['output_queries'])\n",
    "input_df.to_csv(f\"data/{prefix}/input_data.csv\", encoding='utf-8', sep=',')\n",
    "output_df.to_cfsv(f\"data/{prefix}/output_data.csv\", encoding='utf-8', sep=',')\n",
    "\n",
    "raw_input_df = pd.DataFrame(raw_input_texts, columns=['input_queries'])\n",
    "raw_output_df = pd.DataFrame(raw_output_texts, columns=['output_queries'])\n",
    "raw_input_df.to_csv(f\"data/{prefix}/raw_input_data.csv\", encoding='utf-8', sep=';')\n",
    "raw_output_df.to_csv(f\"data/{prefix}/raw_output_data.csv\", encoding='utf-8', sep=';')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load inputs & outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_texts = pd.read_csv(f'data/{prefix}/input_data.csv')\n",
    "print(f\"input_texts shape: {input_texts.shape}\")\n",
    "output_texts = pd.read_csv(f'data/{prefix}/output_data.csv')\n",
    "print(f\"output_texts shape: {output_texts.shape}\")\n",
    "raw_input_texts = pd.read_csv(f'data/{prefix}/raw_input_data.csv', sep=';')\n",
    "print(f\"raw_input_texts shape: {raw_input_texts.shape}\")\n",
    "raw_output_texts = pd.read_csv(f'data/{prefix}/raw_output_data.csv', sep=';')\n",
    "print(f\"raw_output_texts shape: {raw_output_texts.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorize inputs & outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\users\\user\\documents\\unipi\\ml-query-optimization\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    }
   ],
   "source": [
    "input_vectorizer, input_corpus = text_vectorization(input_df, ['input_queries'], (1, 1))\n",
    "output_vectorizer, output_corpus = text_vectorization(output_df, ['output_queries'], (1, 3))\n",
    "\n",
    "print(\"number of encoder words : \", len(input_vectorizer.vocabulary_.keys()))\n",
    "print(\"number of decoder words : \", len(output_vectorizer.vocabulary_.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of encoder words :  30\n",
      "number of decoder words :  464\n"
     ]
    }
   ],
   "source": [
    "raw_input_vectorizer, raw_input_corpus = text_vectorization(raw_input_df, ['input_queries'], (1, 1))\n",
    "raw_output_vectorizer, raw_output_corpus = text_vectorization(raw_output_df, ['output_queries'], (1, 3))\n",
    "\n",
    "print(\"number of raw encoder words : \", len(raw_input_vectorizer.vocabulary_.keys()))\n",
    "print(\"number of raw decoder words : \", len(raw_output_vectorizer.vocabulary_.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train embedding models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove_vectors = get_glove_vectors()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "input_encoder = EmbeddingsModel()\n",
    "input_encoder.build(input_corpus, glove_vectors)\n",
    "output_encoder = EmbeddingsModel()\n",
    "output_encoder.build(output_corpus, glove_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_input_encoder = EmbeddingsModel()\n",
    "raw_input_encoder.build(raw_input_corpus, glove_vectors)\n",
    "raw_output_encoder = EmbeddingsModel()\n",
    "raw_output_encoder.build(raw_output_corpus, glove_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save embedding models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir -p data/embedding_models/training\n",
    "!mkdir -p data/embedding_models/testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = f'data/embedding_models/{prefix}/input_encoder'\n",
    "input_encoder.model.save(filename)\n",
    "filename = f'data/embedding_models/{prefix}/output_encoder'\n",
    "output_encoder.model.save(filename)\n",
    "\n",
    "filename = f'data/embedding_models/{prefix}/raw_input_encoder'\n",
    "raw_input_encoder.model.save(filename)\n",
    "filename = f'data/embedding_models/{prefix}/raw_output_encoder'\n",
    "raw_output_encoder.model.save(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot embedding models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_embeddings(input_encoder, 27, f\"Input {prefix} Encoder Embeddings\")\n",
    "plot_embeddings(output_encoder, 38, f\"Output {prefix} Encoder Embeddings\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load embedding models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_encoder = EmbeddingsModel()\n",
    "input_encoder.model = Word2Vec.load(f'data/embedding_models/{prefix}/input_encoder')\n",
    "output_encoder = EmbeddingsModel()\n",
    "output_encoder.model = Word2Vec.load(f'data/embedding_models/{prefix}/output_encoder')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_input_encoder = EmbeddingsModel()\n",
    "raw_input_encoder.model = Word2Vec.load(f'data/embedding_models/{prefix}/raw_input_encoder')\n",
    "raw_output_encoder = EmbeddingsModel()\n",
    "raw_output_encoder.model = Word2Vec.load(f'data/embedding_models/{prefix}/raw_output_encoder')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize inputs & outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_w2v_weights = input_encoder.model.wv.vectors\n",
    "x_vocab_size, x_embedding_size = x_w2v_weights.shape\n",
    "print(\"Input {} vocabulary size: {} - Embedding Dim: {}\".format(prefix, x_vocab_size, x_embedding_size))\n",
    "\n",
    "y_w2v_weights = output_encoder.model.wv.vectors\n",
    "y_vocab_size, y_embedding_size = y_w2v_weights.shape\n",
    "print(\"Output {} vocabulary size: {} - Embedding Dim: {}\".format(prefix, y_vocab_size, y_embedding_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if prefix == 'training':\n",
    "    X = input_texts['input_queries'].values\n",
    "    y = output_texts['output_queries'].values\n",
    "\n",
    "    # tokenize inputs - outputs\n",
    "    x_max_length, x_vocab, X_pad = pad_tokenizer(X)\n",
    "    y_max_length, y_vocab, y_pad = pad_tokenizer(y)\n",
    "\n",
    "    X_train_pad, X_test_pad, y_train_pad, y_test_pad = train_test_split(X_pad, y_pad, test_size=0.2, random_state=42)\n",
    "\n",
    "    raw_X = raw_input_texts['input_queries'].values\n",
    "    raw_y = raw_output_texts['output_queries'].values\n",
    "\n",
    "    # tokenize inputs - outputs\n",
    "    raw_x_max_length, raw_x_vocab, raw_X_pad = pad_tokenizer(raw_X)\n",
    "    raw_y_max_length, raw_y_vocab, raw_y_pad = pad_tokenizer(raw_y)\n",
    "\n",
    "    raw_X_train_pad, raw_X_test_pad, raw_y_train_pad, raw_y_test_pad = train_test_split(raw_X_pad, raw_y_pad, test_size=0.2, random_state=42)\n",
    "\n",
    "else:\n",
    "    X = input_texts['input_queries'].values\n",
    "    y = output_texts['output_queries'].values\n",
    "\n",
    "    x_max_length, x_vocab, X_train_pad = pad_tokenizer(X)\n",
    "    y_max_length, y_vocab, y_train_pad = pad_tokenizer(y)\n",
    "\n",
    "    raw_X = raw_input_texts['input_queries'].values\n",
    "    raw_y = raw_output_texts['output_queries'].values\n",
    "\n",
    "    # tokenize inputs - outputs\n",
    "    raw_x_max_length, raw_x_vocab, raw_X_train_pad = pad_tokenizer(raw_X)\n",
    "    raw_y_max_length, raw_y_vocab, raw_y_train_pad = pad_tokenizer(raw_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LOG_EVERY = 50\n",
    "PATIENCE = 5\n",
    "WAIT = 0\n",
    "BEST = np.Inf\n",
    "ENCODER_BEST_WEIGHTS = None\n",
    "DECODER_BEST_WEIGHTS = None\n",
    "BEST_EPOCH = 0\n",
    "STOPPED_EPOCH = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Metrics functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_tensor = tf.convert_to_tensor(X_train_pad)\n",
    "output_tensor = tf.convert_to_tensor(y_train_pad)\n",
    "\n",
    "buffer_size = len(input_tensor)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "steps_per_epoch = len(input_tensor) // BATCH_SIZE\n",
    "\n",
    "input_tensor_val = tf.convert_to_tensor(X_test_pad)\n",
    "output_tensor_val = tf.convert_to_tensor(y_test_pad)\n",
    "\n",
    "buffer_size_val = len(input_tensor_val)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, output_tensor_val)).shuffle(buffer_size_val)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE)\n",
    "steps_per_epoch_val = len(input_tensor_val) // BATCH_SIZE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create checkpoint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir -p data/weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder = Encoder(len(x_vocab) + 1, get_embedding_matrix(x_vocab, input_encoder.model.wv), x_max_length)\n",
    "decoder = Decoder(len(y_vocab) + 1, get_embedding_matrix(y_vocab, output_encoder.model.wv), y_max_length, 'concat')\n",
    "\n",
    "optimizer = Adam(0.005)\n",
    "\n",
    "checkpoint_dir = 'data/weights/chkp'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "  start_time = time.time()\n",
    "  total_loss = 0.0\n",
    "  total_accuracy = 0.0\n",
    "  enc_hidden = encoder.init_hidden(BATCH_SIZE)\n",
    "\n",
    "  for idx, (input_tensor, target_tensor) in enumerate(dataset.take(steps_per_epoch)):\n",
    "      # print(\"idx: {0}, input_tensor shape: {1}, target_tensor shape: {2}\".format(idx, input_tensor.shape,\n",
    "      #                                                                            output_tensor.shape))\n",
    "      batch_loss, batch_accuracy = train_step(input_tensor, target_tensor, enc_hidden, encoder, decoder, optimizer)\n",
    "      total_loss += batch_loss\n",
    "      total_accuracy += batch_accuracy\n",
    "\n",
    "      if idx % LOG_EVERY == 0:\n",
    "          print(\"Epochs: {} batch {}/{} | batch_accuracy: {:.4f} | batch_loss: {:.4f}\".format(e, idx, steps_per_epoch, batch_accuracy, batch_loss))\n",
    "\n",
    "  train_acc = total_accuracy / steps_per_epoch\n",
    "  train_accuracies.append(train_acc)\n",
    "  train_loss = total_loss / steps_per_epoch\n",
    "  train_losses.append(train_loss)\n",
    "  checkpoint.save(file_prefix = checkpoint_dir)\n",
    "  print(\"Epoch: {} | Training accuracy over epoch: {:.4f} | Training loss over epoch: {:.4f}\".format(e, float(train_acc), float(train_loss)))\n",
    "\n",
    "  total_loss = 0.0\n",
    "  total_accuracy = 0.0\n",
    "  for idx, (input_tensor_val, target_tensor_val) in enumerate(dataset_val.take(steps_per_epoch_val)):\n",
    "    batch_loss, batch_accuracy = test_step(input_tensor_val, target_tensor_val, enc_hidden, encoder, decoder)\n",
    "    total_loss += batch_loss\n",
    "    total_accuracy += batch_accuracy\n",
    "\n",
    "  val_acc = total_accuracy / steps_per_epoch_val\n",
    "  val_accuracies.append(val_acc)\n",
    "  val_loss = total_loss / steps_per_epoch_val\n",
    "  val_losses.append(val_loss)\n",
    "  print(\"Epoch: {} | Validation acc: {:.4f} | Validation loss: {:.4f}\".format(e, float(val_acc), float(val_loss)))\n",
    "  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "  # if e % 2 == 0:\n",
    "      # print(\"Epochs: {}/{} | total_loss: {:.4f} | total_accuracy: {:.4f}\".format(\n",
    "      #     e, EPOCHS, total_loss / steps_per_epoch, total_accuracy / steps_per_epoch))\n",
    "  #    print(\"Epochs: {}/{} | total_loss: {:.4f}\".format(e, EPOCHS, total_loss / steps_per_epoch))\n",
    "\n",
    "  # The early stopping strategy: stop the training if `val_loss` does not\n",
    "  # decrease over a certain number of epochs.\n",
    "\n",
    "  if np.less(val_loss, BEST):\n",
    "    BEST = val_loss\n",
    "    WAIT = 0\n",
    "    BEST_EPOCH = e\n",
    "    # Record the best weights if current results is better (less).\n",
    "    ENCODER_BEST_WEIGHTS = encoder.get_weights()\n",
    "    DECODER_BEST_WEIGHTS = decoder.get_weights()\n",
    "  else:\n",
    "    WAIT += 1\n",
    "    if WAIT >= PATIENCE:\n",
    "      STOPPED_EPOCH = e\n",
    "      encoder.stop_training = True\n",
    "      decoder.stop_training = True\n",
    "      print(f\"Early stopping activated! Restoring model weights from the end of the (best) epoch: {BEST_EPOCH}.\")\n",
    "      encoder.set_weights(ENCODER_BEST_WEIGHTS)\n",
    "      decoder.set_weights(DECODER_BEST_WEIGHTS)\n",
    "      checkpoint.save(file_prefix = checkpoint_dir)\n",
    "      break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/weights/results.pkl\", \"wb\") as f:\n",
    "  results = {\n",
    "    'train_losses': train_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies\n",
    "  }\n",
    "  pickle.dump(results, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/weights/results.pkl\", \"rb\") as f:\n",
    "  results = pickle.load(f)\n",
    "results.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_losses = [float(loss) for loss in results['train_losses']]\n",
    "train_accuracies = [float(loss) for loss in results['train_accuracies']]\n",
    "val_losses = [float(loss) for loss in results['val_losses']]\n",
    "val_accuracies = [float(loss) for loss in results['val_accuracies']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(train_losses, color=\"C0\", label='train')\n",
    "ax1.plot(val_losses, color=\"C1\", label='validation')\n",
    "\n",
    "ax1.grid(which=\"major\", axis=\"both\")\n",
    "ax1.set_ylabel(\"loss\")\n",
    "ax1.set_xlabel(\"epoch\")\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "ax2.plot(train_accuracies, color=\"C0\", label='train')\n",
    "ax2.plot(val_accuracies, color=\"C1\", label='validation')\n",
    "\n",
    "ax2.grid(which=\"major\", axis=\"both\")\n",
    "ax2.set_ylabel(\"accuracy\")\n",
    "ax2.set_xlabel(\"epoch\")\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "fig2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}